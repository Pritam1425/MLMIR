{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Modelv1.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"wPDUbACiijpw","colab_type":"code","colab":{}},"source":["import os\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import roc_auc_score,accuracy_score\n","import h5py\n","import tensorflow as tf"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3jAxmwGOqV3h","colab_type":"code","outputId":"129b258e-d213-4f19-bfff-5e73e88ad670","executionInfo":{"status":"ok","timestamp":1564750219899,"user_tz":-330,"elapsed":159877,"user":{"displayName":"PRITAM SARKAR","photoUrl":"","userId":"10941419793073735429"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HhPoQcfxn1bk","colab_type":"code","colab":{}},"source":["Xtrain = []\n","Ytrain = []\n","Xtest = []\n","Ytest = []"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_VIx-BZphaUA","colab_type":"code","colab":{}},"source":["def wave_frontend(x, is_training):\n","    '''Function implementing the front-end proposed by Lee et al. 2017.\n","       Lee, et al. \"Sample-level Deep Convolutional Neural Networks for Music\n","       Auto-tagging Using Raw Waveforms.\"\n","       arXiv preprint arXiv:1703.01789 (2017).\n","    - 'x': placeholder whith the input.\n","    - 'is_training': placeholder indicating weather it is training or test\n","    phase, for dropout or batch norm.\n","    '''\n","    initializer = tf.contrib.layers.variance_scaling_initializer()\n","    conv0 = tf.layers.conv1d(inputs=x,\n","                             filters=64,\n","                             kernel_size=3,\n","                             strides=3,\n","                             padding=\"valid\",\n","                             activation=tf.nn.relu,\n","                             kernel_initializer=initializer)\n","    bn_conv0 = tf.layers.batch_normalization(conv0, training=is_training)\n","\n","    conv1 = tf.layers.conv1d(inputs=bn_conv0,\n","                             filters=64,\n","                             kernel_size=3,\n","                             strides=1,\n","                             padding=\"valid\",\n","                             activation=tf.nn.relu,\n","                             kernel_initializer=initializer)\n","    bn_conv1 = tf.layers.batch_normalization(conv1, training=is_training)\n","    pool_1 = tf.layers.max_pooling1d(bn_conv1, pool_size=3, strides=3)\n","\n","    conv2 = tf.layers.conv1d(inputs=pool_1,\n","                             filters=64,\n","                             kernel_size=3,\n","                             strides=1,\n","                             padding=\"valid\",\n","                             activation=tf.nn.relu,\n","                             kernel_initializer=initializer)\n","    bn_conv2 = tf.layers.batch_normalization(conv2, training=is_training)\n","    pool_2 = tf.layers.max_pooling1d(bn_conv2, pool_size=3, strides=3)\n","\n","    conv3 = tf.layers.conv1d(inputs=pool_2,\n","                             filters=128,\n","                             kernel_size=3,\n","                             strides=1,\n","                             padding=\"valid\",\n","                             activation=tf.nn.relu,\n","                             kernel_initializer=initializer)\n","    bn_conv3 = tf.layers.batch_normalization(conv3, training=is_training)\n","    pool_3 = tf.layers.max_pooling1d(bn_conv3, pool_size=3, strides=3)\n","\n","    conv4 = tf.layers.conv1d(inputs=pool_3,\n","                             filters=128,\n","                             kernel_size=3,\n","                             strides=1,\n","                             padding=\"valid\",\n","                             activation=tf.nn.relu,\n","                             kernel_initializer=initializer)\n","    bn_conv4 = tf.layers.batch_normalization(conv4, training=is_training)\n","    pool_4 = tf.layers.max_pooling1d(bn_conv4, pool_size=3, strides=3)\n","\n","    conv5 = tf.layers.conv1d(inputs=pool_4,\n","                             filters=128,\n","                             kernel_size=3,\n","                             strides=1,\n","                             padding=\"valid\",\n","                             activation=tf.nn.relu,\n","                             kernel_initializer=initializer)\n","    bn_conv5 = tf.layers.batch_normalization(conv5, training=is_training)\n","    pool_5 = tf.layers.max_pooling1d(bn_conv5, pool_size=3, strides=3)\n","\n","    conv6 = tf.layers.conv1d(inputs=pool_5,\n","                             filters=256,\n","                             kernel_size=3,\n","                             strides=1,\n","                             padding=\"valid\",\n","                             activation=tf.nn.relu,\n","                             kernel_initializer=initializer)\n","    bn_conv6 = tf.layers.batch_normalization(conv6, training=is_training)\n","    pool_6 = tf.layers.max_pooling1d(bn_conv6, pool_size=3, strides=3)\n","\n","    return tf.expand_dims(pool_6, [3])\n","def spec_frontend(x, is_training, config, num_filt):\n","    '''Function implementing the proposed spectrogram front-end.\n","    - 'route_out': is the output of the front-end, and therefore the input of\n","        this function.\n","    - 'is_training': placeholder indicating weather it is training or test\n","        phase, for dropout or batch norm.\n","    - 'config': dictionary with some configurable parameters like: number of\n","        output units - config['numOutputNeurons'] or number of frequency bins\n","        of the spectrogram config['setup_params']['yInput']\n","    - 'num_filt': multiplicative factor that controls the number of filters\n","        for every filter shape.\n","    '''\n","    initializer = tf.contrib.layers.variance_scaling_initializer()\n","    y_input = config['setup_params']['yInput']\n","    input_layer = tf.expand_dims(x, 3)\n","\n","    # padding only time domain for an efficient 'same' implementation\n","    # (since we pool throughout all frequency afterwards)\n","    input_pad_7 = tf.pad(input_layer,\n","                         [[0, 0], [3, 3], [0, 0], [0, 0]],\n","                         \"CONSTANT\")\n","    input_pad_3 = tf.pad(input_layer,\n","                         [[0, 0], [1, 1], [0, 0], [0, 0]],\n","                         \"CONSTANT\")\n","\n","    # [TIMBRE] filter shape 1: 7x0.9f\n","    conv1 = tf.layers.conv2d(inputs=input_pad_7,\n","                             filters=num_filt,\n","                             kernel_size=[7, int(0.9 * y_input)],\n","                             padding=\"valid\",\n","                             activation=tf.nn.relu,\n","                             kernel_initializer=initializer,reuse=tf.AUTO_REUSE)\n","    bn_conv1 = tf.layers.batch_normalization(conv1, training=is_training)\n","    pool1 = tf.layers.max_pooling2d(inputs=bn_conv1,\n","                                    pool_size=[1, bn_conv1.shape[2]],\n","                                    strides=[1, bn_conv1.shape[2]])\n","    p1 = tf.squeeze(pool1, [2])\n","\n","    # [TIMBRE] filter shape 2: 3x0.9f\n","    conv2 = tf.layers.conv2d(inputs=input_pad_3,\n","                             filters=num_filt*2,\n","                             kernel_size=[3, int(0.9 * y_input)],\n","                             padding=\"valid\",\n","                             activation=tf.nn.relu,\n","                             kernel_initializer=initializer,reuse=tf.AUTO_REUSE)\n","    bn_conv2 = tf.layers.batch_normalization(conv2, training=is_training)\n","    pool2 = tf.layers.max_pooling2d(inputs=bn_conv2,\n","                                    pool_size=[1, bn_conv2.shape[2]],\n","                                    strides=[1, bn_conv2.shape[2]])\n","    p2 = tf.squeeze(pool2, [2])\n","\n","    # [TIMBRE] filter shape 3: 1x0.9f\n","    conv3 = tf.layers.conv2d(inputs=input_layer,\n","                             filters=num_filt*4,\n","                             kernel_size=[1, int(0.9 * y_input)],\n","                             padding=\"valid\",\n","                             activation=tf.nn.relu,\n","                             kernel_initializer=initializer,reuse=tf.AUTO_REUSE)\n","    bn_conv3 = tf.layers.batch_normalization(conv3, training=is_training)\n","    pool3 = tf.layers.max_pooling2d(inputs=bn_conv3,\n","                                    pool_size=[1, bn_conv3.shape[2]],\n","                                    strides=[1, bn_conv3.shape[2]])\n","    p3 = tf.squeeze(pool3, [2])\n","\n","    # [TIMBRE] filter shape 3: 7x0.4f\n","    conv4 = tf.layers.conv2d(inputs=input_pad_7,\n","                             filters=num_filt,\n","                             kernel_size=[7, int(0.4 * y_input)],\n","                             padding=\"valid\",\n","                             activation=tf.nn.relu,\n","                             kernel_initializer=initializer,reuse=tf.AUTO_REUSE)\n","    bn_conv4 = tf.layers.batch_normalization(conv4, training=is_training)\n","    pool4 = tf.layers.max_pooling2d(inputs=bn_conv4,\n","                                    pool_size=[1, bn_conv4.shape[2]],\n","                                    strides=[1, bn_conv4.shape[2]])\n","    p4 = tf.squeeze(pool4, [2])\n","\n","    # [TIMBRE] filter shape 5: 3x0.4f\n","    conv5 = tf.layers.conv2d(inputs=input_pad_3,\n","                             filters=num_filt * 2,\n","                             kernel_size=[3, int(0.4 * y_input)],\n","                             padding=\"valid\",\n","                             activation=tf.nn.relu,\n","                             kernel_initializer=initializer,reuse=tf.AUTO_REUSE)\n","    bn_conv5 = tf.layers.batch_normalization(conv5, training=is_training)\n","    pool5 = tf.layers.max_pooling2d(inputs=bn_conv5,\n","                                    pool_size=[1, bn_conv5.shape[2]],\n","                                    strides=[1, bn_conv5.shape[2]])\n","    p5 = tf.squeeze(pool5, [2])\n","\n","    # [TIMBRE] filter shape 6: 1x0.4f\n","    conv6 = tf.layers.conv2d(inputs=input_layer,\n","                             filters=num_filt * 4,\n","                             kernel_size=[1, int(0.4 * y_input)],\n","                             padding=\"valid\",\n","                             activation=tf.nn.relu,\n","                             kernel_initializer=initializer,reuse=tf.AUTO_REUSE)\n","    bn_conv6 = tf.layers.batch_normalization(conv6, training=is_training)\n","    pool6 = tf.layers.max_pooling2d(inputs=bn_conv6,\n","                                    pool_size=[1, bn_conv6.shape[2]],\n","                                    strides=[1, bn_conv6.shape[2]])\n","    p6 = tf.squeeze(pool6, [2])\n","\n","    # [TEMPORAL-FEATURES] - average pooling + filter shape 7: 165x1\n","    pool7 = tf.layers.average_pooling2d(inputs=input_layer,\n","                                        pool_size=[1, y_input],\n","                                        strides=[1, y_input])\n","    pool7_rs = tf.squeeze(pool7, [3])\n","    conv7 = tf.layers.conv1d(inputs=pool7_rs,\n","                             filters=num_filt,\n","                             kernel_size=165,\n","                             padding=\"same\",\n","                             activation=tf.nn.relu,\n","                             kernel_initializer=initializer,reuse=tf.AUTO_REUSE)\n","    bn_conv7 = tf.layers.batch_normalization(conv7, training=is_training)\n","\n","    # [TEMPORAL-FEATURES] - average pooling + filter shape 8: 128x1\n","    pool8 = tf.layers.average_pooling2d(inputs=input_layer,\n","                                        pool_size=[1, y_input],\n","                                        strides=[1, y_input])\n","    pool8_rs = tf.squeeze(pool8, [3])\n","    conv8 = tf.layers.conv1d(inputs=pool8_rs,\n","                             filters=num_filt*2,\n","                             kernel_size=128,\n","                             padding=\"same\",\n","                             activation=tf.nn.relu,\n","                             kernel_initializer=initializer,reuse=tf.AUTO_REUSE)\n","    bn_conv8 = tf.layers.batch_normalization(conv8, training=is_training)\n","\n","    # [TEMPORAL-FEATURES] - average pooling + filter shape 9: 64x1\n","    pool9 = tf.layers.average_pooling2d(inputs=input_layer,\n","                                        pool_size=[1, y_input],\n","                                        strides=[1, y_input])\n","    pool9_rs = tf.squeeze(pool9, [3])\n","    conv9 = tf.layers.conv1d(inputs=pool9_rs,\n","                             filters=num_filt*4,\n","                             kernel_size=64,\n","                             padding=\"same\",\n","                             activation=tf.nn.relu,\n","                             kernel_initializer=initializer,reuse=tf.AUTO_REUSE)\n","    bn_conv9 = tf.layers.batch_normalization(conv9, training=is_training)\n","\n","    # [TEMPORAL-FEATURES] - average pooling + filter shape 10: 32x1\n","    pool10 = tf.layers.average_pooling2d(inputs=input_layer,\n","                                         pool_size=[1, y_input],\n","                                         strides=[1, y_input])\n","    pool10_rs = tf.squeeze(pool10, [3])\n","    conv10 = tf.layers.conv1d(inputs=pool10_rs,\n","                              filters=num_filt*8,\n","                              kernel_size=32,\n","                              padding=\"same\",\n","                              activation=tf.nn.relu,\n","                              kernel_initializer=initializer,reuse=tf.AUTO_REUSE)\n","    bn_conv10 = tf.layers.batch_normalization(conv10, training=is_training)\n","\n","    # concatenate all feature maps\n","    pool = tf.concat([p1, p2, p3, p4, p5, p6, bn_conv7, bn_conv8, bn_conv9,\n","                      bn_conv10], 2)\n","    return tf.expand_dims(pool, 3)\n","\n","\n","def backend(route_out, is_training, config, num_units):\n","    '''Function implementing the proposed back-end.\n","    - 'route_out': is the output of the front-end, and therefore the input of\n","        this function.\n","    - 'is_training': placeholder indicating weather it is training or test\n","        phase, for dropout or batch norm.\n","    - 'config': dictionary with some configurable parameters like: number of\n","        output units - config['numOutputNeurons'] or number of frequency bins\n","        of the spectrogram config['setup_params']['yInput']\n","    - 'num_units': number of units/neurons of the output dense layer.\n","    '''\n","    initializer = tf.contrib.layers.variance_scaling_initializer()\n","\n","    # conv layer 1 - adapting dimensions\n","    conv1 = tf.layers.conv2d(inputs=route_out,\n","                             filters=256,\n","                             kernel_size=[7, route_out.shape[2]],\n","                             padding=\"valid\",\n","                             activation=tf.nn.relu,\n","                             name='1cnnOut',\n","                             kernel_initializer=initializer,reuse=tf.AUTO_REUSE)\n","    bn_conv1 = tf.layers.batch_normalization(conv1, training=is_training)\n","    bn_conv1_t = tf.transpose(bn_conv1, [0, 1, 3, 2])\n","\n","    # conv layer 2 - residual connection\n","    bn_conv1_pad = tf.pad(bn_conv1_t,\n","                          [[0, 0], [3, 3], [0, 0], [0, 0]],\n","                          \"CONSTANT\")\n","    conv2 = tf.layers.conv2d(inputs=bn_conv1_pad,\n","                             filters=256,\n","                             kernel_size=[7, bn_conv1_pad.shape[2]],\n","                             padding=\"valid\",\n","                             activation=tf.nn.relu,\n","                             name='2cnnOut',\n","                             kernel_initializer=initializer,reuse=tf.AUTO_REUSE)\n","    conv2_t = tf.transpose(conv2, [0, 1, 3, 2])\n","    bn_conv2 = tf.layers.batch_normalization(conv2_t, training=is_training)\n","    res_conv2 = tf.add(bn_conv2, bn_conv1_t)\n","\n","    # temporal pooling\n","    pool1 = tf.layers.max_pooling2d(inputs=res_conv2, pool_size=[2, 1],\n","                                    strides=[2, 1], name='poolOut')\n","\n","    # conv layer 3 - residual connection\n","    bn_conv4_pad = tf.pad(pool1,\n","                          [[0, 0], [3, 3], [0, 0], [0, 0]],\n","                          \"CONSTANT\")\n","    conv5 = tf.layers.conv2d(inputs=bn_conv4_pad,\n","                             filters=256,\n","                             kernel_size=[7, bn_conv4_pad.shape[2]],\n","                             padding=\"valid\",\n","                             activation=tf.nn.relu,\n","                             name='3cnnOut',\n","                             kernel_initializer=initializer,reuse=tf.AUTO_REUSE)\n","    conv5_t = tf.transpose(conv5, [0, 1, 3, 2])\n","    bn_conv5 = tf.layers.batch_normalization(conv5_t, training=is_training)\n","    res_conv5 = tf.add(bn_conv5, pool1)\n","\n","    # global pooling: max and average\n","    max_pool2 = tf.reduce_max(res_conv5, axis=1)\n","    avg_pool2, var_pool2 = tf.nn.moments(res_conv5, axes=[1])\n","    pool2 = tf.concat([max_pool2, avg_pool2], 2)\n","    flat_pool2 = tf.contrib.layers.flatten(pool2)\n","\n","    # output - 1 dense layer with droupout\n","    flat_pool2_dropout = tf.layers.dropout(flat_pool2, rate=0.5,\n","                                           training=is_training)\n","    dense = tf.layers.dense(inputs=flat_pool2_dropout,\n","                            units=num_units,\n","                            activation=tf.nn.relu,\n","                            kernel_initializer=initializer,reuse=tf.AUTO_REUSE)\n","    bn_dense = tf.layers.batch_normalization(dense, training=is_training)\n","    dense_dropout = tf.layers.dropout(bn_dense, rate=0.5, training=is_training)\n","    return tf.layers.dense(inputs=dense_dropout,\n","                           activation=tf.sigmoid,\n","                           units=config['numOutputNeurons'],\n","                           kernel_initializer=initializer,reuse=tf.AUTO_REUSE)\n","\n","\n","def build_model(x, is_training, config):\n","    '''Function implementing an example of how to build a model with the\n","        functions above.\n","    - 'x': placeholder whith the input.\n","    - 'is_training': placeholder indicating weather it is training or test\n","        phase, for dropout or batch norm.\n","    - 'config': dictionary with some configurable parameters like: number of\n","        output units - config['numOutputNeurons'] or number of frequency bins\n","        of the spectrogram config['setup_params']['yInput']\n","    '''\n","    # The following line builds the model that achieved better results in our\n","    # experiments. It is based on a spectrogram front-end (num_filters=16)\n","    # with 500 output units in the dense layer.\n","    return backend(spec_frontend(x, is_training, config, 1), is_training,\n","                   config, 50)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TJ6TJHr4_Scu","colab_type":"code","colab":{}},"source":["tf.reset_default_graph()\n","os.chdir('gdrive/My Drive/MyMIR/final_Xtrain')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CJ5cM256sY_r","colab_type":"code","outputId":"7603d55f-4b25-4892-f395-094058e7f12d","executionInfo":{"status":"error","timestamp":1563555015289,"user_tz":-330,"elapsed":19677,"user":{"displayName":"PRITAM SARKAR","photoUrl":"","userId":"10941419793073735429"}},"colab":{"base_uri":"https://localhost:8080/","height":714}},"source":["for i in range(0,12):\n","  if i<10:\n","    Xtrain = np.load('data'+str(i)+'.npy')\n","    os.chdir('../final_Ytrain')\n","    Ytrain = np.load('Y'+str(i)+'.npy')\n","    os.chdir('../final_Xtrain')\n","  elif i == 10:\n","    Xtrain = np.load('dataA.npy')\n","    os.chdir('../final_Ytrain')\n","    Ytrain = np.load('YA.npy')\n","    os.chdir('../final_Xtrain')\n","  elif i == 11:\n","    Xtrain = np.load('dataB.npy')\n","    os.chdir('../final_Ytrain')\n","    Ytrain = np.load('YB.npy')\n","    os.chdir('../final_Xtrain')\n","  else:\n","    Xtrain = np.load('dataC.npy')\n","    os.chdir('../final_Ytrain')\n","    Ytrain = np.load('YC.npy')\n","    os.chdir('../final_Xtrain')\n","  #Xtrain = Xtrain.reshape(Xtrain.shape[0],Xtrain.shape[1],Xtrain.shape[2],1)\n","  #Xval = Xval.reshape(Xval.shape[0],Xval.shape[1],Xval.shape[2],1)\n","  config = {'numOutputNeurons':50,'setup_params':{'yInput':10}}\n","  print(Ytrain.shape)\n","  Xtrain = np.float32(Xtrain)\n","  Ytrain = np.float32(Ytrain)\n","  Ypredict = build_model(Xtrain,1,config)\n","  with  tf.Session() as ses:\n","    ses.run(tf.initialize_all_variables())\n","    Ypredict = ses.run(Ypredict)#(Xtrain, Ytrain,batch_size=32,epochs=10,verbose=1,validation_split=0.1,validation_data = (Xval,Yval),shuffle=True)tf.device('/device:GPU:0') and\n","    print(roc_auc_score(Ytrain, Ypredict))\n","    #print(accuracy_score(Ytrain, Ypredict))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(1108, 50)\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0719 16:50:09.727943 140026153273216 lazy_loader.py:50] \n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","W0719 16:50:11.989313 140026153273216 deprecation.py:323] From <ipython-input-4-d9fdeeea9ada>:112: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.keras.layers.Conv2D` instead.\n","W0719 16:50:12.231842 140026153273216 deprecation.py:323] From <ipython-input-4-d9fdeeea9ada>:113: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n","W0719 16:50:12.312709 140026153273216 deprecation.py:323] From <ipython-input-4-d9fdeeea9ada>:116: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.MaxPooling2D instead.\n"],"name":"stderr"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-0f0cf646e06e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mXtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mYtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m   \u001b[0mYpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m   \u001b[0;32mwith\u001b[0m  \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-d9fdeeea9ada>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(x, is_training, config)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;31m# experiments. It is based on a spectrogram front-end (num_filters=16)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;31m# with 500 output units in the dense layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     return backend(spec_frontend(x, is_training, config, 1), is_training,\n\u001b[0m\u001b[1;32m    335\u001b[0m                    config, 50)\n","\u001b[0;32m<ipython-input-4-d9fdeeea9ada>\u001b[0m in \u001b[0;36mspec_frontend\u001b[0;34m(x, is_training, config, num_filt)\u001b[0m\n\u001b[1;32m    123\u001b[0m                              \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"valid\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                              \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                              kernel_initializer=initializer,reuse=tf.AUTO_REUSE)\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mbn_conv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_normalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     pool2 = tf.layers.max_pooling2d(inputs=bn_conv2,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/convolutional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(inputs, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[1;32m    422\u001b[0m       \u001b[0m_reuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m       _scope=name)\n\u001b[0;32m--> 424\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m     \"\"\"\n\u001b[0;32m-> 1479\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_subclass_implementers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m           \u001b[0;31m# Wrapping `call` function in autograph to allow for dynamic control\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1879\u001b[0m       \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1880\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1881\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1882\u001b[0m     \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m     \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         dtype=self.dtype)\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m       self.bias = self.add_weight(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, partitioner, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mgetter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections_arg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         aggregation=aggregation)\n\u001b[0m\u001b[1;32m    385\u001b[0m     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1494\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1237\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m   def _get_partitioned_variable(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    560\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m   def _get_partitioned_variable(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    512\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     synchronization, aggregation, trainable = (\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    867\u001b[0m         raise ValueError(\"Trying to share variable %s, but specified shape %s\"\n\u001b[1;32m    868\u001b[0m                          \u001b[0;34m\" and found shape %s.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m                          (name, shape, found_var.get_shape()))\n\u001b[0m\u001b[1;32m    870\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0mdtype_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Trying to share variable conv2d/kernel, but specified shape (3, 9, 1, 2) and found shape (7, 9, 1, 1)."]}]},{"cell_type":"code","metadata":{"id":"GTKucJ2qs_nQ","colab_type":"code","colab":{}},"source":["tempx = [['dataD.npy','YD.npy'],['dataE.npy','YE.npy'],['dataF.npy','YF.npy']]\n","for tple in tempx:\n","  Xtestp,Ytestp = tple\n","  os.chdir('../Xtrain')\n","  Xtest = np.load(Xtestp)\n","  os.chdir('../Ytrain')\n","  Ytest = np.load(Ytestp)\n","  predicted = build_model(Xtest,0,config)\n","  with tf.Session() as ses:\n","    ses.run(Ypredict)\n","    print(roc_auc_score(Ytest, predicted))\n","    print(accuracy_score(Ytest, predicted))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"abq_4Ck2EC8S","colab_type":"code","colab":{}},"source":["os.listdir()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S6gQCm9tDQvq","colab_type":"code","outputId":"a5112254-e483-4404-fec3-18da09d7d16b","executionInfo":{"status":"error","timestamp":1563555476472,"user_tz":-330,"elapsed":1377,"user":{"displayName":"PRITAM SARKAR","photoUrl":"","userId":"10941419793073735429"}},"colab":{"base_uri":"https://localhost:8080/","height":230}},"source":["os.chdir('gdrive/My Drive/MyMIR/final_Xtrain')\n","data = []\n","Y = []\n","for i in range(0,9):\n","  if i<10:\n","    Xtrain = np.load('data'+str(i)+'.npy')\n","    os.chdir('../final_Ytrain')\n","    #Ytrain = np.load('Y'+str(i)+'.npy')\n","    os.chdir('../final_Xtrain')\n","  elif i == 10:\n","    Xtrain = np.load('dataA.npy')\n","    os.chdir('../final_Ytrain')\n","    #Ytrain = np.load('YA.npy')\n","    os.chdir('../final_Xtrain')\n","  elif i == 11:\n","    Xtrain = np.load('dataB.npy')\n","    os.chdir('../final_Ytrain')\n","    #Ytrain = np.load('YB.npy')\n","    os.chdir('../final_Xtrain')\n","  #Xtrain = Xtrain.reshape(Xtrain.shape[0],Xtrain.shape[1],Xtrain.shape[2],1)\n","  #Xval = Xval.reshape(Xval.shape[0],Xval.shape[1],Xval.shape[2],1)\n","  data.append(Xtrain)\n","  #Y.append(Ytrain)\n","  print('Xtrain'+str(i)+' Saved')\n","np.save('data',data)\n","#np.save('Y',Y)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-d3007c81cbd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gdrive/My Drive/MyMIR/final_Xtrain'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"]}]},{"cell_type":"code","metadata":{"id":"UfJwTBR-D7dR","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}